<!doctype html>
<script src="jquery.js"></script>
<script src="data.js"></script>

<!-- Changes below this can cause problems -->

<html lang="en-us">
<head>
    <meta charset="UTF-8">
    <title id="mainPageTitleforSEO">Accepted Workshops</title>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Rubik:ital,wght@0,300..900;1,300..900&display=swap"
          rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Bai+Jamjuree:ital,wght@0,200;0,300;0,400;0,500;0,600;0,700;1,200;1,300;1,400;1,500;1,600;1,700&family=Rubik:ital,wght@0,300..900;1,300..900&display=swap"
          rel="stylesheet">
    <link href="build.css" rel="stylesheet">
    <link rel="icon" type="image/png" href="/favicon-48x48.png" sizes="48x48"/>
    <link rel="icon" type="image/svg+xml" href="/favicon.svg"/>
    <link rel="shortcut icon" href="/favicon.ico"/>
    <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"/>
    <meta name="apple-mobile-web-app-title" content="RLC"/>
    <link rel="manifest" href="/site.webmanifest"/>
    <meta id="seodescription" name="description" content="">

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Bai+Jamjuree:ital,wght@0,200;0,300;0,400;0,500;0,600;0,700;1,200;1,300;1,400;1,500;1,600;1,700&family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&family=Rubik:ital,wght@0,300..900;1,300..900&display=swap"
          rel="stylesheet">
</head>

<body>


<div class="container  mx-auto ">
    <div class="flex flex-col min-h-screen justify-between">
        <div>
            <div class="m-2 grid grid-cols-3  items-center  rounded-md mt-4 p-2 border-0 border-rldarkblue-900">
                <div class="p-2 w-full rounded-md">
                    <div class="hidden lg:block max-w-60">
                        <img alt="Company logo" src="data/logos/rlc-logo.svg"/>
                    </div>
                    <div class="block pt-1 lg:hidden max-w-60">
                        <img alt="Company logo" src="data/logos/rlc-logo.svg"/>
                    </div>
                </div>
                <div></div>
                <div id="largeMenu" class="p-2 m-1 w-full hidden lg:block rounded-md">

                </div>
                <div id="collapsedMenu" class="p-2 m-1 w-full block lg:hidden rounded-md ">
                    <div class="relative flex flex-row-reverse">
                        <svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5"
                             stroke="currentColor"
                             class="size-10 p-1 font-rubik text-xl m-1 text-rldarkblue-900 hover:text-rldarkblue-500 hover:cursor-pointer"
                             onclick="showMenu()">
                            <path stroke-linecap="round" stroke-linejoin="round"
                                  d="M3.75 5.25h16.5m-16.5 4.5h16.5m-16.5 4.5h16.5m-16.5 4.5h16.5"/>
                        </svg>
                        <div id="collapsedMenuItems"
                             class="absolute top-10 z-10  hidden w-40 shadow-md bg-rlgrey/100 backdrop-blur-md rounded-md p-4 m-4 grid grid-cols-1 border-1 border-black">

                        </div>
                    </div>
                </div>
            </div>
            <div class="relative bg-rlgrey/50   rounded-md  ">
                <div class="p-4">
                    <h1 class="text-3xl p-2 m-2 text-center font-rubik text-blue mb-4">Accepted Workshops</h1>
                    <div id="acceptedWorkshops" class="grid grid-cols-1  gap-6">
                        
                    </div>
                </div>
            </div>
        </div>


        <div class="grid grid-cols-2 items-center  ">
            <div id="footerText"
                 class="p-2 m-1 w-full rounded-md text-rldarkblue-900 font-roboto text-xs sm:text-base">
            </div>
            <div class="p-2 m-1 w-full">
                <div class="flex flex-row-reverse  p-2 ml-auto max-w-60 ">
                    <div><img alt="Company logo" class="p-1 m-1 w-60" src="data/logos/rlc-logo.svg"/></div>
                </div>
            </div>
        </div>
    </div>
</div>

</body>
<!--Javascript code for generating the HTML-->
<script src="menu.js"></script>
<script>

list_of_accepted_workshops = [new Workshop("Workshop on Programmatic Reinforcement Learning", "https://prl-workshop.github.io/", "This workshop explores the emerging paradigm of programmatic representations—symbolic programs, code-based policies, and rule-based abstractions—to address fundamental challenges in reinforcement learning (RL) such as interpretability, generalizability, efficiency, and safety. By bringing together experts in RL, program synthesis, and code generation, the workshop aims to discuss specific topics, including but not limited to, using programs as policies (LEAPS, Code as Policies, HPRL, RoboTool, Carvalho et al. 2024), reward functions (Eureka, Language2Reward, Text2Reward), task generators (GenSim), or environment models (WorldCoder, Code World Models) for RL. This paradigm bridging RL and programmatic representations enables human-understandable reasoning, reduces reliance on massive data-driven models, and promotes modularity, fostering progress toward verifiable and robust agents across virtual and real-world applications."),
    new Workshop("Reinforcement Learning and Video Games", " https://sites.google.com/view/rlvg-workshop-2025/home", " Video games and reinforcement learning (RL) have a rich and intertwined history. Video games have long served as challenging benchmarks that have driven major advances in the field, pushing the boundaries of RL in diverse, complex environments like Atari, Dota 2, StarCraft, and Minecraft. In turn, RL has fueled innovation in video game development, enabling more realistic and adaptable non-player characters (NPCs), as well as enhancing quality assurance (QA) and procedural content generation. This workshop celebrates the 10th anniversary of the Deep Q-Network Atari Nature paper, a pivotal milestone that established video games as a cornerstone for RL research. Over the past decade, video games have catalyzed breakthroughs in RL, yet as these important benchmarks become increasingly saturated, it is time to reflect on the lessons learned and explore what is still missing. What are the open research problems in RL that can be isolated and addressed through novel video game benchmarks? What is preventing RL from being more widely applied in modern video games?"),
    new Workshop("Inductive Biases in Reinforcement Learning", "https://sites.google.com/view/ibrl-workshop/home", "Inductive biases encode prior knowledge about the world and play a crucial role in shaping the learning process in reinforcement learning (RL) agents. In particular, they allow for boosting the performance of RL algorithms by incorporating assumptions and steering them toward the most plausible solutions in the policy search. The Inductive Biases in Reinforcement Learning (IBRL) workshop aims to investigate the role of inductive biases in modern RL methods, analyzing their impact on the learning procedure from various perspectives and contexts. It will assess the limitations of current approaches and explore novel designs to address these gaps toward novel sample-efficient RL algorithms and more robust, general, and adaptable RL agents. The IBRL workshop aims to foster collaboration between RL subfields and promote fruitful discussion on different inductive biases by featuring targeted sessions covering a wide range of topics. Specific topics include but are not limited to abstractions and structured policies, generalization, relational biases and representations, learning biases for robotics, and future directions."),
    new Workshop("Coordination and Cooperation in Multi-Agent Reinforcement Learning", "https://sites.google.com/view/cocomarl2025/", "Multi-agent reinforcement learning (MARL) has emerged as an important approach for enabling autonomous agents to learn and adapt in complex, dynamic environments. With growing relevance in domains like robotic warehousing, space traffic management, and autonomous driving, MARL presents unique challenges and opportunities in both fundamental research and practical applications. The Coordination and Cooperation in Multi-Agent Reinforcement Learning (CoCoMARL) workshop centers on MARL problems that require cooperation and coordination between agents, an area with many open challenges. Our goal is to bring together researchers and practitioners to explore recent advances, challenges, and applications of MARL. CoCoMARL highlights topics including cooperation and coordination methods, inter-agent communication, the integration of large language models (LLMs), real-world applications across various domains, the use of game theory, and safety and reliability in critical environments."), 
    new Workshop("Practical Insights into RL for Real Systems", "https://rl4rs.github.io/RL4RS/", "State-of-the-art reinforcement learning (RL) methods have demonstrated their ability to solve difficult and complex problems in areas such as video games and natural language processing. These successes suggest that RL algorithms are powerful enough to tackle other complex problems, including the control and optimization of real-world systems. Some of these systems, like supply chains and power grids, are currently managed by well established approaches. However, these methods can become suboptimal when faced with issues of scale, uncertainty, and complexity. Other systems, such as nuclear fusions reactors or autonomous robots, still lack sufficiently powerful control algorithms. In both cases, RL presents a promising solution. However, there are still many challenges to applying RL for such systems, including issues related to performance, evaluation, and certifiability, which depend on the nature and context of each problem. In this workshop, we aim to answer the question: Under which conditions can RL be the right solution for a real system? Unlike previous workshops on deploying RL in the real world, our focus of this workshop will not only be on technical approaches to address these issues, although they are welcome. Instead, we aim to provide a space for RL practitioners, who have chosen RL as the path for their problems, to discuss their experiences and offer practical insights into the implementation of RL in such settings."), 
    new Workshop("The Causal Reinforcement Learning Workshop", "#", "Reinforcement Learning (RL) and Causal Inference have developed as largely independent fields, yet they share a fundamental connection: both aim to model and reason about how actions influence outcomes in uncertain environments. Despite their intertwined nature, the interaction between these two disciplines has been limited, leaving many fundamental questions unresolved. Recently, there has been growing interest in bridging these fields to extend the decision-making paradigm beyond the standard (PO)MDPs to accommodate potential confounding biases in real-world environments and enhance RL algorithms' generalization, robustness, and sample efficiency. Causal concepts have the potential to improve RL in various ways: enabling better credit assignment, guiding exploration, improving transportability across tasks, and facilitating better explanations. Conversely, RL offers an interactive framework for causal decision-making, realizing the concept of interventions in real-world complex environments. The CausalRL Workshop aims to bring together researchers at the intersection of RL and Causality to explore new opportunities, challenges, and recent advances. Through invited talks, contributed presentations, and discussions, we seek to foster collaboration and define key research directions that could shape the future of causal reinforcement learning."), 
new Workshop("Workshop on RL Beyond Rewards: Ingredients for Developing Generalist Agents", "https://rlbrew2-workshop.github.io/", " Reinforcement Learning (RL) has traditionally focused on maximizing rewards. However, intelligent agents often rely on reward-free interactions and diverse environmental signals to form abstractions that facilitate rapid adaptation. Recent RL research has begun leveraging reward-free transitions—available through exploratory interactions or expert datasets—to increase decision-making efficiency and task specification. However, unlike in vision or language modeling, RL still lacks scalable methods for learning generalizable representations from unlabeled data. Additionally, difficulties in specifying reward functions have led researchers toward alternative signals, such as human demonstrations, preferences, and implicit feedback. This workshop seeks to advance beyond traditional reward-centric RL by exploring methods like intrinsic motivation, skill discovery, predictive and contrastive representation learning, and leveraging human-centric signals. Building upon recent progress, including foundational models employing scalable alternative signals, the workshop aims to bridge theoretical insights and practical applications, fostering collaborations toward creating more versatile, adaptive decision-making agents."), 
new Workshop("Finding the Frame: A Workshop for Examining Conceptual Frameworks in RL", "https://sites.google.com/view/findingtheframe/home", "Scientific research is often framed within paradigms of concepts, definitions, axioms, and assumptions — RL is no exception. While such details usually go unchallenged in the pursuit of specific objectives, this workshop brings them into sharp focus. In this workshop, we examine the conceptual foundations of RL and their impact on framing problems. Examining RL from this lens has the potential to open up new territory, test our historical beliefs, and ultimately guide our future research endeavors in productive directions. This is the second iteration of the workshop, building off of last year's workshop.")]

for (let i = 0; i < list_of_accepted_workshops.length; i++) {
    const workshop = list_of_accepted_workshops[i];
    const workshopElement = document.createElement("div");
    workshopElement.classList.add("bg-rllightblue-50", "border-rldarkblue-900", "rounded-lg");
    workshopElement.innerHTML = `
        <div class="p-4 bg-rllightblue-50 rounded-md">
                <h2 class="text-2xl font-rubik text-rldarkblue-900">${workshop.name}
                </h2>
                 ${workshop.link === '#' ? 
                        `<p class="text-gray-600">Coming soon</p>` : 
                        `<a href="${workshop.link}" class="text-blue hover:text-rldarkblue-500">Website</a>`
                    }
            <p class=" mb-2"><span class="font-bold">Abstract:</span> ${workshop.description}</p>
        </div>
    `;
    document.getElementById("acceptedWorkshops").appendChild(workshopElement);
}



</script>
<div class="flex z-10 z-20 z-30 z-40 left-0 right-0  p-6 m-6 p-3 m-3 w-60 hidden group-hover:block hover:cursor-pointer group   flex-row-reverse p-1 font-rubik  m-1 text-rldarkblue-900 hover:text-rldarkblue-500 hover:cursor-pointer w-full text-sm sm:text-base font-rubik text-xl ml-4 mr-4 text-rldarkblue-900 hover:text-rldarkblue-500 hover:cursor-pointer"></div>
</html>

