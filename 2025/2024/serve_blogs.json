[{"content":"<h1>The why behind RLC</h1>\n\n<p>We\u2019re so excited to launch the first version of the RLC, the new archival conference focused exclusively on reinforcement learning. There\u2019s been a lot of attention, and we figured this would be a good moment to explain why we think now is the right moment to launch a new RL conference and talk a bit about what makes RLC different from other ML conferences. Besides the obvious, that we\u2019re RL-focused, there\u2019s a whole host of things that we\u2019re trying to make this a uniquely worthwhile conference to submit to and attend.</p>\n\n<h2>Why a new archival RL conference?</h2>\n\n<p>This is the question we get the most. It\u2019s a few things that make the RL community, and RL papers, somewhat distinct and that we think benefit from a conference being archival:\n- RL is not a monolithic topic; it\u2019s a lot of subgroups roughly pursuing a similar topic. Having an archival RL conference lets us have sessions organized more appropriately to subgroups in the RL community.\n- It is beneficial to RL practitioners and researchers to have a dedicated venue to find top-quality work with top quality reviewing that follows high standards defined by the RL community.\n- In the longer-term, it will help the RL community develop those standards for reproducibility, best practices, etc. that are necessary to address challenges that are more important to RL.</p>\n\n<p>Finally, a yearly archival RL conference provides an opportunity to create a coherent RL community. The  RL community is gigantic and even though we all work on so many different problems, there are a lot of unifying themes in the work and concepts we can learn from each other. Researchers lucky enough to attend prior RLDM events have been overwhelmingly enthusiastic about it, which suggests that there\u2019s a real need for a yearly event that brings the community together outside of the general ML conferences. This community building is also important because RL research is growing in size and complexity, which is resulting in larger papers and more tools needed to support research efforts.</p>\n\n<h2>Why should you submit?</h2>\n\n<p>The conference is new, yet we are already getting significant interest for submission details and where to submit work. The pragmatic case for submission is this: you submit to conferences to (1) make folks aware of your work and get feedback on it and (2) you attend conferences to have interesting conversations and learn about the state-of-the-art. We believe that there\u2019s no better way to draw attention to an RL paper than submitting to RLC. In this first iteration, submitting your paper to RLC and receiving a spotlight or poster is a way to get it in front of a really large fraction of the RL community. These important connections are much more difficult at larger more general conferences. Due to their size, this will not occur except for a few papers when submitting it to a 15000 person conference like NeurIPS. The larger a conference is, the more power-law effects come into play and so a lot of focus gets placed on a smaller number of papers. </p>\n\n<p>There are a few more reasons we\u2019d like to highlight. Our review process (discussed below) places an extremely high technical bar for the inclusion of a paper. As such, we think having a paper accepted to RLC will constitute a mark of paper quality.  </p>\n\n<h2>What we\u2019re doing differently</h2>\n\n<p>Because we\u2019re a new conference, we can take some exciting risks and try to fix long-standing issues we\u2019ve seen in the ML community. The key change is creating a technically rigorous, but quite different and efficient review process that should set a high technical bar and help provide more useful reviews!</p>\n\n<h3>Review process</h3>\n\n<p>Everyone understands that the review process in ML is becoming challenging with increasingly overburdened reviewers starting to submit slipshod reviews. Rather than trying to fix the process with another change atop things, the RLC program chairs (Martha White, Adam White, and Feyral Behbahani) have come up with a new procedure that pares things down:</p>\n\n<ul>\n<li>Rather than asking reviewers to judge the novelty or impact of an approach, which we think is something the research community will do on its own through citations, we\u2019re just asking reviewers to judge the technical correctness of the work. This simplifies the job of reviewing and hopefully eases the review burden. A lower review burden means we can ask technical reviewers to provide even more thorough reviews.</li>\n<li>Rather than ensure that you receive many reviews, we\u2019re focusing on a smaller number of high-quality reviews. Reviews will come from more senior PhD students focused on technical correctness paired with a senior reviewer who will double-check the review and provide a more high-level perspective on the relationship of the paper to the field. We are also empowering the senior to throw out low quality technical reviews that simply add noise to the process.</li>\n<li>As to the above, we are setting a high technical bar for a paper being correct. More information on this will come out soon.</li>\n</ul>\n\n<p>We hope that providing good feedback makes the RLC submission process more informative and useful. Whether your paper gets in or not, you\u2019ll receive detailed feedback from experts that allows you to improve the quality of your paper (just in time for NeurIPS if your paper doesn\u2019t get in!). As a reviewer, you actually get to learn how to review from the senior reviewer. For much more detail on the review process there\u2019ll be another blog post coming soon discussing all this in more detail and explaining how we came to this setup.</p>\n\n<h2>Some questions that come up frequently</h2>\n\n<h3>What about RLDM?</h3>\n\n<p>We love RLDM and think it\u2019s great. At the same time, we think it\u2019s important to have something archival for the reasons outlined above.</p>\n\n<h3>What about EWRL?</h3>\n\n<p>We think EWRL is great too and are actively exploring ways to bring RLC and EWRL together. At the same time, we think it\u2019s important to have a venue that is both archival and free to move from continent to continent.</p>\n\n<h3>Are you worried this will split up RL from the ML community?</h3>\n\n<p>There are over 10,000 RL papers written every year. More than enough of these will go to other ML conferences such that RL still remains a major part of those conferences while RLC can still provide a focused spotlight on good RL papers.</p>\n","file_path":"/blogs/why_rlc.html","title":"What's exciting about RLC?"},{"content":"<h1>Reinforcement Learning Conference: an exploration step in reviewing</h1>\n\n<p>RLC decisions came out recently; congratulations to all the accepted papers and thank you to everyone who submitted a paper. The aim of this post is to shed light on the new processes that we hope will result in a better research publishing and conference experience. Rather than choosing a particular target acceptance right, we chose to focus on technical rigor and checking \u201cdoes this paper\u2019s evidence match its claims''. In addition, inspired by TMLR, senior area chairs were empowered to further evaluate if RLC represented an appropriate and non-trivial audience for the work (and thus judge novelty and positioning of the work as well). Finally, we aspired to uphold high scholarly standards, meaning clarity, writing, and polish were held to a high standard. Judgements on significance, focus on state-of-the-art claims and more generally \u201ctaste making\u201d were discouraged.</p>\n\n<p>How did this new approach work out? We are excited to report that we wound up with a 40% acceptance rate and 115 accepted papers, which is right in line with early versions of other ML conferences. Having said that, it\u2019s a new process so we wanted to take this time to go over some of its interesting aspects, clear up some misconceptions, and discuss future improvements we are considering.</p>\n\n<h2>The Process</h2>\n\n<figure style=\"display: flex;max-width: 100%;flex-direction: column;\">\n    <img style=\"max-width: 100%;\" src=\"/static/images/rlc-review-process.png\" alt=\"Review Process\">\n    <figcaption style=\"margin-top: 10px;margin-top: 10px;font-size: 1rem;color: #555;\">\n    Figure 1: The purple arrows indicate the PC\u2019s influence over, but not control over, similar to the critic\u2019s feedback to an actor in <a href=\"http://www.incompleteideas.net/book/ebook/node66.html\">classic RL diagrams</a>.\n    </figcaption>\n</figure>\n\n<p>Our approach to improving reviewing was to reduce workload for reviewers, but ask for increased effort and quality from every reviewer per paper. In particular, we asked slightly less of our technical reviewers (who were typically more junior researchers, but not always), by having them only assess technical correctness, and relied on our senior reviewers to handle both technical correctness as well as more subtle factors like relevance to the field. The combination of these two reviews was then passed to a senior area chair (SAC) who synthesized the two, read and evaluated the paper (effectively a third reviewer), and then made a final call. These SACs we handpicked\u2014senior folks with years of experience post PhD and known to be both generous to new ideas and have excellent judgment.</p>\n\n<p>The program chairs were active at all stages of the process: giving comments and suggestions to improve both technical and senior reviews, discussing and troubleshooting issues with the SACs, and helping make final decisions about borderline papers. [In our system, if an SAC rated a paper as borderline they were communicating to the program chairs they needed help making the final call on the paper. All such cases were discussed by the PC.] Each PC member was assigned as a \u201cbuddy\u201d to several SACs. The PC buddy provided a direct communication channel for questions, concerns and discussions about each paper, for each SAC. The full review process is summarized in the diagram above. Overall, each paper was read by four people and the decision was based on information from all four people.</p>\n\n<p>The final decision process worked as follows. The SAC made the final decision based on (1) reading the paper, (2) the two reviews, (3) discussion with the reviewers and the PC, and (4) (optional) direct Q&amp;A with the authors. If the decision was reject, weak accept, or accept that decision was mapped to reject, accept, or accept respectively. If the SAC marked the paper as borderline, then the PC made the final decision. This is the process visualized in the figure above.</p>\n\n<p>Note that at the scale we\u2019re operating at, we were able to handpick many of the reviewers, but still, many needed course correction. The program chairs, SACs, and many senior reviewers went to great lengths to identify and improve low-quality technical reviews. First, the technical reviewers were reminded of the reviewer guidelines and asked to improve their reviews. If the review was not updated satisfactorily, then a new technical reviewer was added. In several cases, the PC noted parts of technical reviews that were judged to be violating the spirit of the review process. We followed a similar process for attracting and improving senior reviews, including adding another senior reviewer in some cases. By large, this group was well known to the PC and organizers and the quality of these reviews was quite good. Like any conference, there were uncommunicative senior reviewers\u2014and SACs\u2014but nothing worth noting. Someone from the PC read every technical and senior review submitted to RLC. </p>\n\n<p>This new process was an experiment\u2014no system is perfect\u2014but we believe this system is better for RLC than the standard approach. Our primary goals were to reduce workload, increase review quality, and focus on correctness and quality rather than taste. This was not easy and required a new process. Furthermore, this was a big change for our reviewers, who are accustomed to the typical review style that focuses primarily on the perception of significance and impact. It will take multiple iterations to fine-tune this system, but comments from the community suggest we are on the right path. We are excited about the papers that are accepted to RLC and looking forward to meeting all the authors. We will also continue to iterate and improve the system based on the results of this first experiment and feedback from the community.</p>\n\n<h2>What went well</h2>\n\n<p>We were happy with several aspects of this new review process. The feedback we received from folks in the community that our focus, on claims matching evidence and correctness, was seen as a breath of fresh air and a good way forward. Most importantly, we tried something very new and we now have data and comments to improve the process going forward.</p>\n\n<p>The engagement and feedback provided to technical reviewers early on in the process by PCs, SACs, and SRs was effective. Many technical reviewers actually engaged and updated their reviews significantly. In cases where there was a lack of engagement, we assigned new reviewers from our pool of emergency reviewers, ensuring the review process continued smoothly. As mentioned, this required significant engagement from PCs throughout the process, which we will need to rethink to allow RLC to grow. </p>\n\n<p>The detailed guidelines for TRs, SRs, and SACs were helpful to many. These guidelines provided instructions and expectations, helping to maintain consistency and quality across all reviews. Naturally, these documents can be sharpened and consolidated, but they provided a guidance for both reviewers and authors. Sharing these documents with authors before the submission deadline provided transparency and perhaps helped align submissions with the conference\u2019s goals.</p>\n\n<p>The design of the review process was based on the principle of subtraction and appears to have had the desired effect. Typically, when things are not working well, people add new things: more reviewers, more tasks for reviewers and ACs, etc. This creates much more work, but often does not improve things. Subtraction, stripping things back to the barebones, can be a more effective alternative. This was the goal of introducing technical reviews, removing traditional author response, and reducing the number of reviewers overall. We believe the workload for technical reviewers and senior reviewers was much less than usual. For SACs, who typically must read reviews, discussions, author response, and long dialogs in OpenReview for up to five reviews per-paper, the workload was similar but now the SAC\u2019s time was put into reading the papers and writing more detailed meta reviews. Overall, we think workload was reduced and decision making accuracy was at least maintained and likely improved. </p>\n\n<p>Finally, the introduction of PC buddies for SACs proved to be quite helpful. This system made it clear who the point of contact was for each SAC, facilitating better communication and support throughout the review process. We received constructive comments from several SACs on how to do this even better next year!</p>\n\n<h2>Looking forwards</h2>\n\n<p>There are a number of changes that we are contemplating in the next edition of RLC.</p>\n\n<h3>Continuing to improve reviewer guidelines</h3>\n\n<p>Many technical reviewers, instructed only to check for technical correctness, submitted almost one-line reviews of the form \u201cthis is correct.\u201d In many cases, the paper under review was excellent and this one-liner corresponded to an extremely thorough review! However, a paper author would not be able to tell if this review was thorough or not. In future years, we will likely have more detailed TR review instructions that make their work more visible. </p>\n\n<h3>Reviewing software and libraries</h3>\n\n<p>Software and libraries are a key part of RL in practice but require a slightly different review process. We\u2019re now starting the process of figuring out what a good, thorough review cycle would look like for software contributions.</p>\n\n<h3>Continuing to refine and grow our pool of reviewers</h3>\n\n<p>Initially, there were many low-quality technical reviews. We cannot be sure why. Some may have found the instructions confusing; some might not have dedicated enough time to the process; many clearly did not read the instructions and template matched to reviews they typically write. Others we know either lacked the ability to check correctness or refused to do so. Many reviewers who previously won reviewing rewards from major conferences produced some of the lowest quality reviews and never responded to comments or suggestions from the PC. Whatever the reason, we were quite surprised at the quality of some of the technical reviews; improving them required significant effort from the PC\u2014and that will not scale as RLC grows. </p>\n\n<p>We have to have an honest conversation with our community about mechanism design (rewarding good reviews) and training people on how to review papers. If folks do not take reviewing seriously and with an attitude of generosity towards the work, then the whole process breaks down. We can do better; we must do better! </p>\n\n<h3>Review town hall</h3>\n\n<p>We will likely host a town hall to discuss the process and address questions raised. We will be soliciting questions in advance of the conference so that we can address and discuss the most common ones. We are also very excited to get people from the community more involved and look forward to discussing ideas to improve this process for the future.</p>\n","file_path":"/blogs/review_process.html","title":"Reinforcement Learning Conference: an exploration step in reviewing"},{"content":"<h1>On RLC\u2019s Outstanding Paper Awards</h1>\n\n<p>At RLC the \u201cOutstanding Paper Award\u201d will be different from what is traditionally done in machine learning conferences. We will not award papers for being the overall \u201cbest\u201d papers in a conference; instead we will award papers for making significant contributions to specific aspects of research. We believe such an approach will be more inclusive\u2013it will celebrate the diverse types of scientific contribution one can make in the field, and it will provide more equal opportunities for different types of papers to be awarded. The idea is to award papers for excelling in what they aimto accomplish.</p>\n\n<p>It is commonly agreed that best paper awards are not necessarily a good predictor of impact in the future. We designed this award system with this in mind. Aligned with the RLC reviewing guidelines, this system tries not to award papers for the popularity of the topic they investigate, nor for their perceived novelty or potential future impact. </p>\n\n<p>We are also hoping that this process will decouple scores and the likelihood of an award. Currently, in many systems, a single non-committal review can drastically reduce the chances of a paper receiving an award.</p>\n\n<p>This system was designed to recognize and promote the diversity in our field. Papers that make meaningful contributions to real-world problems can be awarded in the same way that a purely theoretical paper can. Papers that close a gap in our understanding, or that report interesting negative results can also be recognized, or those that introduce a completely new perspective to the field. Finally, we also believe that the computational resources one has access to also should not change the likelihood of a paper receiving an award; thus there is a category for excelling in doing empirical research frugally. </p>\n\n<p>There will be six award categories, they are the following (in alphabetical order):</p>\n\n<ul>\n<li>Outstanding Paper Award on Applications of Reinforcement Learning</li>\n<li>Outstanding Paper Award on Empirical Reinforcement Learning Research </li>\n<li>Outstanding Paper Award on Empirical Resourcefulness in Reinforcement Learning</li>\n<li>Outstanding Paper Award on Pioneering Vision in Reinforcement Learning</li>\n<li>Outstanding Paper Award on Scientific Understanding in Reinforcement Learning</li>\n<li>Outstanding Paper Award on the Theory of Reinforcement Learning</li>\n</ul>\n\n<p>A more detailed description of each category is available at the end of this post. Importantly, <em>no award is more prestigious than the other.</em></p>\n\n<h2>And how is this going to work?</h2>\n\n<p>In the review form, the technical reviewer and the senior reviewer will have to answer six yes/no questions in the form \u201cShould this paper be nominated for the Outstanding Paper Award on X?\u201d. These are not exclusive, so one paper can be nominated for more than one award. If the paper is nominated for any award, the nominator will be asked to write an additional paragraph justifying the nomination. </p>\n\n<p>As discussed above, at RLC, the overall scores a paper receives will not necessarily determine whether the paper will receive an award or not. A paper might be accepted with a \u201cWeak accept\u201d but it may have excelled in a specific aspect captured by an award. We will encourage reviewers to nominate such papers. This approach is an attempt to decorrelate the noise from the review process from the award itself. As a concrete example, a paper might be accepted with a \u201cWeak accept\u201d because the reviewers recognize that the paper provides important theoretical results that tackles a major theoretical question in reinforcement learning, but these new results do not necessarily lead to major performance improvements (whether such a paper should receive a Strong Accept is beyond the scope of this post, we are simply acknowledging the noisiness of the process here). Such a paper should probably be nominated for the <em>Outstanding Paper Award on the Theory of Reinforcement Learning</em>. </p>\n\n<p>Importantly, more than one paper can potentially receive the same award in a given year, and there might be years in which an award is not given. </p>\n\n<p>We hope you find this interesting!</p>\n\n<p><em>RLC Awards co-chairs</em></p>\n\n<p><em>Roberta Raileanu &amp; Marlos C. Machado</em></p>\n\n<h2>Description of Award Categories (Alphabetical Order)</h2>\n\n<h3>Outstanding Paper Award on Applications of Reinforcement Learning</h3>\n\n<p><em>This award aims to acknowledge papers that demonstrate substantial progress on the application of reinforcement learning to complex, real-world problems. This award seeks to highlight groundbreaking work formulating real-world problems using the reinforcement learning framework, introducing a new application domain or challenge to reinforcement learning, or developing reinforcement learning methods that make significant progress on practical scenarios.  The papers should display a notable level of practical utility and uphold a high standard of scientific rigor.</em></p>\n\n<h3>Outstanding Paper Award on Empirical Reinforcement Learning Research</h3>\n\n<p><em>This award recognizes papers that make significant contributions to the empirical aspects of reinforcement learning research. Examples include addressing fundamental practical challenges in reinforcement learning, introducing new empirical practices, methodologies, benchmarks, evaluation metrics, and visualization techniques, and providing tools and frameworks that will further enable empirical research. These papers should show a high standard of practical relevance and experimental rigor.</em></p>\n\n<h3>Outstanding Paper Award on Empirical Resourcefulness in Reinforcement Learning</h3>\n\n<p><em>This award honors papers that demonstrate resourcefulness in empirical research. These are papers that overcome the high computational cost of empirical research in reinforcement learning in ingenious ways, promoting more frugal empirical research. Examples include showcasing original, cost-effective methodologies, and resource-efficient experimental designs. The papers should embody high standards of creativity and practicality without sacrificing experimental rigor.</em></p>\n\n<h3>Outstanding Paper Award on Pioneering Vision in Reinforcement Learning</h3>\n\n<p><em>This award highlights papers that stand out with their forward-thinking vision and blue-sky ideas in the field of reinforcement learning. The papers awarded in this category will present groundbreaking, visionary ideas, theories, or techniques in reinforcement learning, potentially reshaping current perspectives or opening new avenues for research and applications. The papers must demonstrate originality, creativity, and the potential to inspire transformative advancements in reinforcement learning.</em></p>\n\n<h3>Outstanding Paper Award on Scientific Understanding in Reinforcement Learning</h3>\n\n<p><em>This award celebrates papers that significantly advance scientific understanding in the domain of reinforcement learning. It encourages the development of well-founded, clear understanding of the behavior of existing algorithms or the nuances of different problem formulations or different environments. Awarded papers will fill gaps in our understanding of the field; they will bring clarity to unexplored aspects of existing algorithms, they will provide evidence to dispute common assumptions, or they will better justify common practices in the field. They should also demonstrate excellence in scientific rigor and clarity of exposition, with very well-defined claims.</em></p>\n\n<h3>Outstanding Paper Award on the Theory of Reinforcement Learning</h3>\n\n<p><em>This award acknowledges papers that provide exceptional theoretical contributions to the field of reinforcement learning. Examples include theoretical unifications, new theoretical frameworks or formalisms, mathematical models, results, and theoretical insights into existing RL practices. The papers must exhibit a high level of technical proficiency and innovation.</em></p>\n","file_path":"/blogs/awards_process.html","title":"On RLC\u2019s Outstanding Paper Awards"},{"content":"<h1>Announcing the RLC 2024 Outstanding Paper Awards</h1>\n\n<p>We are honoured to announce the winners of the Outstanding Paper Awards at the First Reinforcement Learning Conference. As we outlined in another blog post, papers are awarded at RLC based on specific aspects of their contribution. We describe the process we used to select the awarded papers after. </p>\n\n<p>This year\u2019s awards consist of seven papers, one per category. The awarded papers are listed below in alphabetical order by award name. Congratulations to all the authors! </p>\n\n<p><strong>Outstanding Paper Award on Applications of RL</strong>\nM. Vasco, T. Seno, K. Kawamoto, K. Subramanian, P. R. Wurman, and P. Stone: <em>A Super-human Vision-based Reinforcement Learning Agent for Autonomous Racing in Gran Turismo</em>. <a href=\"https://rlj.cs.umass.edu/2024/papers/Paper213.html\">Link to paper</a>.</p>\n\n<p><em>This paper demonstrates for the first time that RL agents can achieve super-human performance in a high-fidelity racing simulator using only local features at execution time. Until now, this has only been possible by relying on global features or privileged information, which is unfeasible in practice. Hence, this work constitutes substantial progress in the application of RL for autonomous racing.</em></p>\n\n<p><strong>Outstanding Paper Award on Empirical Methods in RL Research</strong>\nK. Javed, A. Sharifnassab, and R. S. Sutton: <em>SwiftTD: A Fast and Robust Algorithm for Temporal Difference Learning</em>. <a href=\"https://rlj.cs.umass.edu/2024/papers/Paper111.html\">Link to paper</a>.</p>\n\n<p><em>This paper proposes a new online temporal-difference algorithm for the prediction problem in RL with strong empirical performance when using linear function approximation. This paper stood out for its thoroughness, breadth, and depth of experiments that went above and beyond to provide empirical evidence of how the algorithm works. Examples include ablating all the relevant components, visualizing predictions and credit assigned to each pixel, running extensive hyperparameter searches for all baselines, and more.</em></p>\n\n<p><strong>Outstanding Paper Award on Empirical Resourcefulness in RL</strong>\nA. Raffin, O. Sigaud, J. Kober, A. Albu-Schaeffer, J. Silv\u00e9rio, and F. Stulp: <em>An Open-Loop Baseline for Reinforcement Learning Locomotion Tasks</em>. <a href=\"https://rlj.cs.umass.edu/2024/papers/Paper18.html\">Link to paper</a>.</p>\n\n<p><em>This paper challenges common practices in RL by introducing a simple, low-cost method that is competitive with state-of-the-art deep RL algorithms on locomotion tasks. The proposed method generates periodic joint motions using simple oscillators. It performs strongly both in simulation and transfer to a real-world quadruped robot, further highlighting the practicality of this approach. This paper raises an interesting discussion about the trade-offs between cost, complexity, and generality when using deep RL, making it a valuable contribution to Empirical Resourcefulness in RL.</em></p>\n\n<p><strong>Outstanding Paper Award on Pioneering Vision in RL</strong>\nC. Cousins, K. Asadi, E. Lobo, and M. Littman. <em>On Welfare-Centric Fair Reinforcement Learning</em>. <a href=\"https://rlj.cs.umass.edu/2024/papers/Paper133.html\">Link to paper</a>.</p>\n\n<p><em>This paper introduces a new framework for fair reinforcement learning, which allows for different societal ideals of fairness to be encoded through a welfare function rather than optimizing for a specific definition of fairness. The paper stood out for its clear exposition, motivation, and comprehensive theoretical results. This paper\u2019s pioneering vision on fairness in RL opens the door to new research directions where society and other relevant parties agree on the notion of fairness rather than the algorithmic designer.</em></p>\n\n<p><strong>Outstanding Paper Award on Scientific Understanding in RL</strong>\nM. Suau, M. T. J. Spaan, and F. A. Oliehoek. Bad Habits: <em>Policy Confounding and Out-of-Trajectory Generalization in RL</em>. <a href=\"https://rlj.cs.umass.edu/2024/papers/Paper216.html\">Link to paper</a>.</p>\n\n<p><em>This paper furthers our scientific understanding of why RL agents struggle to generalize to new scenarios at test time, paving the way to developing more robust RL algorithms. The paper characterizes the phenomenon of policy confounding through the lens of causality, whereby when following specific trajectories, RL agents can learn behaviours based on spurious correlations (between observations and rewards) because the policy is confounded with the data. The paper shows that on-policy algorithms can learn representations that are sufficient for the trajectory induced by the optimal policy but do not necessarily generalize well to new states, making agents non-robust to changes in the environment\u2019s dynamics.</em></p>\n\n<p><strong>Outstanding Paper Award on Support Tools for RL Research</strong>\nD. Corsi, D. Camponogara, and A. Farinelli: <em>Aquatic Navigation: A Challenging Benchmark for Deep Reinforcement Learning</em>. <a href=\"https://rlj.cs.umass.edu/2024/papers/Paper131.html\">Link to paper</a>.</p>\n\n<p><em>This paper develops a simulator for aquatic navigation using real-world data and proposes it as a new benchmark for RL algorithms. This environment poses a challenge to RL algorithms due to its unpredictable and non-stationary dynamics induced by complex fluid dynamics. Reinforcement learning frequently benefits from the often unpraised work of designing new simulators that capture different facets of the problems we care about, and this environment could very well be another one. Designing RL algorithms for aquatic navigation is a novel application that can have significant practical interest, making this work a valuable contribution to Support Tools for RL Research.</em></p>\n\n<p><strong>Outstanding Paper Award on the Theory of RL</strong>\nW. Xu, S. Dong, and B. Van Roy. <em>Posterior Sampling for Continuing Environments</em>. <a href=\"https://rlj.cs.umass.edu/2024/papers/Paper277.html\">Link to paper</a>.</p>\n\n<p><em>This paper develops a new exploration method for the understudied continuing RL problem, showing how one can extend the posterior sampling algorithm originally designed for the episodic setting to the continuing setting. This is achieved by showing how one can reinterpret existing methods to resample a new policy at every time step instead of doing so at the beginning of each episode, an approach which is also effective in high-dimensional state spaces. The resampling probability can be used by the agent to dynamically adjust its planning horizon, thus better handling infinite-horizon problems. This paper stood out through its theoretical rigour in adapting algorithms from the episodic to the continuing setting, being one of the first papers to do so, potentially serving as a catalyst for more research on this topic.</em></p>\n\n<h2>Our Process</h2>\n\n<p>We selected these papers very carefully after following the process we designed. We decided to award papers for specific aspects of their contribution because we wanted to disentangle awards from aspects such as perceived novelty, impact, high scores, or the topic of the paper. This means that we were okay with an awarded paper having flaws in aspects that were not considered for that specific award; we were much more focused on identifying papers that did one specific thing really well.</p>\n\n<p>Before describing the process we followed, notice we ended up adding a new category from what we had initially announced. As we reviewed the potential papers to be awarded, we realized that we were conflating papers with good empirical design and papers that supported empirical research. Because of that, we split the general \u201cOutstanding Paper Award on Empirical Reinforcement Learning Research\u201d into \u201cOutstanding Paper Award on Empirical Methods in RL Research\u201d and \u201cOutstanding Paper Award on Support Tools for RL Research\u201d.</p>\n\n<p>With that out of the way, let us describe our process. First, we considered papers with conference organizers (at any level) as authors ineligible for the award, even when those papers were explicitly nominated for an award. We paid close attention to all papers explicitly nominated for an outstanding paper award. Still, we considered all other accepted papers for the award, regardless of the recommendation the area chair gave (Borderline accept, Weak accept, Accept). We chose to do so to give all papers a chance of being recognized with an award and increase robustness by considering alternative opinions (outside the review process). In addition, reviewers may not nominate papers they consider borderline, while our goal is to recognize papers that excel in a particular aspect rather than being flawless across the board. This was also needed to select papers for the two subcategories on \u201cSupport Tools for RL Research\u201d and \u201cEmpirical Methods in RL Research\u201d since reviewers were only provided with a single category on \u201cEmpirical RL Research\u201d meant to encompass both of these. </p>\n\n<p>We selected the awarded papers in a three-stage process where the two of us did the work independently. At every stage, we would independently come up with a list of nominations, and we would then meet to discuss and decide which papers would go to the next stage. In the first stage, we read the abstracts of all the accepted papers and the meta-reviews, and we selected approximately 50 of those papers (out of a total of 113 accepted papers) for a more careful analysis. In the second stage, we additionally read each paper's reviews and looked at key aspects of the paper itself, coming up with a shortlist of 2-3 papers per category. We read each of these remaining 20 papers, specifically considering the category they were being considered for. As a purely hypothetical example, we would be comfortable awarding a paper on the Theory of RL that has very strong theoretical results but has flaws in its empirical evaluation. We wouldn\u2019t award a paper on Empirical Methods in RL with a weak empirical design. Fortunately, at the end of this long process, we had come up, independently, with the exact same paper to be awarded in each category!</p>\n\n<p>Five of the seven awarded papers received an \u201cAccept\u201d recommendation, and two received a \u201cBorderline Accept\u201d from the area chair. Only three had received a nomination from the reviewers.</p>\n\n<p>Our approach allowed us to award a much more diverse set of papers, and we genuinely hope that it will help the community recognize a wider range of papers as valuable contributions. This process allowed us to award fantastic papers that would have a much harder time being recognized in more traditional review processes. Papers with small and large-scale experiments, with and without theoretical results, and with extremely simple and very complex ideas were all awarded here, and we believe this diversity can only strengthen our community.</p>\n\n<p>One more time, congratulations to all the authors!</p>\n\n<p>Below, you can find the updated list of award categories:</p>\n\n<p><em>*Outstanding Paper Award on Empirical Methods in Reinforcement Learning Research *</em>\n<em>This award recognizes papers that make significant contributions to the empirical methods of reinforcement learning research. Examples include addressing fundamental practical challenges in reinforcement learning or introducing new empirical practices or methodologies. These papers should show a high standard of practical relevance and experimental rigour.</em></p>\n\n<p><strong>Outstanding Paper Award on Support Tools for Reinforcement Learning Research</strong>\n<em>This award recognizes papers that make significant contributions to support tools for reinforcement learning research. Examples include introducing new environments, datasets, benchmarks, evaluation metrics, visualization techniques, or frameworks and tools that will further enable empirical research in reinforcement learning. These papers should show a high standard of practical relevance, accessibility, ease of use and reproducibility.</em></p>\n\n<p><strong>Outstanding Paper Award on Applications of Reinforcement Learning</strong>\n<em>This award aims to acknowledge papers that demonstrate substantial progress in the application of reinforcement learning to complex, real-world problems. This award seeks to highlight groundbreaking work formulating real-world problems using the reinforcement learning framework, introducing a new application domain or challenge to reinforcement learning, or developing reinforcement learning methods that make significant progress on practical scenarios.  The papers should display a notable level of practical utility and uphold a high standard of scientific rigour.</em></p>\n\n<p><strong>Outstanding Paper Award on Empirical Resourcefulness in Reinforcement Learning</strong>\n<em>This award honours papers that demonstrate resourcefulness in empirical research. These are papers that ingeniously overcome the high computational cost of empirical research in reinforcement learning, promoting more frugal empirical research. Examples include showcasing original, cost-effective methodologies and resource-efficient experimental designs. The papers should embody high standards of creativity and practicality without sacrificing experimental rigour.</em></p>\n\n<p><strong>Outstanding Paper Award on Pioneering Vision in Reinforcement Learning</strong>\n*This award highlights papers that stand out with their forward-thinking vision and blue-sky ideas in reinforcement learning. The papers awarded in this category will present groundbreaking, visionary ideas, theories, or techniques in reinforcement learning, potentially reshaping current perspectives or paving new avenues for research and applications. The papers must demonstrate originality, creativity, and the potential to inspire transformative advancements in reinforcement learning. *</p>\n\n<p><strong>Outstanding Paper Award on Scientific Understanding in Reinforcement Learning</strong>\n<em>This award celebrates papers that significantly advance scientific understanding in the domain of reinforcement learning. It encourages the development of a well-founded, clear understanding of the behaviour of existing algorithms or the nuances of different problem formulations or different environments. Awarded papers will fill gaps in our understanding of the field; they will bring clarity to unexplored aspects of existing algorithms, they will provide evidence to dispute common assumptions, or they will better justify common practices in the field. They should also demonstrate excellence in scientific rigour and clarity of exposition with very well-defined claims.</em></p>\n\n<p><strong>Outstanding Paper Award on the Theory of Reinforcement Learning</strong>\n<em>This award acknowledges papers that provide exceptional theoretical contributions to the field of reinforcement learning. Examples include theoretical unifications, new theoretical frameworks or formalisms, mathematical models, results, and theoretical insights into existing RL practices. The papers must exhibit a high level of technical proficiency and innovation.</em></p>\n\n<p>RLC 2024 Award Chairs\nMarlos C. Machado and Roberta Raileanu</p>\n","file_path":"/blogs/paper_awards.html","title":"Announcing the RLC 2024 Outstanding Paper Awards"}]
