<!DOCTYPE html>
<html lang='en'>
<head>
  <meta charset='UTF-8'>
  <meta name='viewport' content='width=device-width, initial-scale=1.0'>
  <script src='https://cdn.tailwindcss.com'></script>
  <title>Conference Schedule</title>
</head>
<body class='p-4 space-y-8 bg-gray-100 font-sans'>
<h2 class='text-2xl font-bold text-gray-800'>Aug 6</h2>
<div class='grid grid-cols-1 md:grid-cols-2 lg:grid-cols-3 gap-4'>
<div class='bg-white p-4 rounded shadow'>
<h3 class='font-semibold text-lg mb-1'>Track 1: RL algorithms</h3>
<ul class='list-disc list-inside text-sm text-gray-700'>
<li>Burning RED: Unlocking Subtask-Driven Reinforcement Learning and Risk-Awareness in Average-Reward Markov Decision Processes</li>
<li>RL$^3$: Boosting Meta Reinforcement Learning via RL inside RL$^2$</li>
<li>Fast Adaptation with Behavioral Foundation Models</li>
<li>Understanding Learned Representations and Action Collapse in Visual Reinforcement Learning</li>
<li>Mitigating Suboptimality of Deterministic Policy Gradients in Complex Q-functions</li>
<li>ProtoCRL: Prototype-based Network for Continual Reinforcement Learning</li>
<li>Offline Reinforcement Learning with Domain-Unlabeled Data</li>
<li>SPEQ: Offline Stabilization Phases for Efficient Q-Learning in High Update-To-Data Ratio Reinforcement Learning</li>
<li>Offline Reinforcement Learning with Wasserstein Regularization via Optimal Transport Maps</li>
<li>Zero-Shot Reinforcement Learning Under Partial Observability</li>
<li>Adaptive Submodular Policy Optimization</li>
</ul></div>
<div class='bg-white p-4 rounded shadow'>
<h3 class='font-semibold text-lg mb-1'>Track 2: RL from human feedback, Imitation Learning</h3>
<ul class='list-disc list-inside text-sm text-gray-700'>
<li>Online Intrinsic Rewards for Decision Making Agents from Large Language Model Feedback</li>
<li>Nonparametric Policy Improvement in Continuous Action Spaces via Expert Demonstrations</li>
<li>DisDP: Robust Imitation Learning via Disentangled Diffusion Policies</li>
<li>Mitigating Goal Misgeneralization via Minimax Regret</li>
<li>Modelling human exploration with light-weight meta reinforcement learning algorithms</li>
<li>Towards Improving Reward Design in RL: A Reward Alignment Metric for RL Practitioners</li>
<li>PAC Apprenticeship Learning with Bayesian Active Inverse Reinforcement Learning</li>
<li>Offline Action-Free Learning of Ex-BMDPs by Comparing Diverse Datasets</li>
<li>One Goal, Many Challenges: Robust Preference Optimization Amid Content-Aware and Multi-Source Noise</li>
<li>Goals vs. Rewards: A Comparative Study of Objective Specification Mechanisms</li>
<li>Reward Distance Comparisons Under Transition Sparsity</li>
</ul></div>
<div class='bg-white p-4 rounded shadow'>
<h3 class='font-semibold text-lg mb-1'>Track 3: Hierarchical RL, Planning algorithms</h3>
<ul class='list-disc list-inside text-sm text-gray-700'>
<li>AVID: Adapting Video Diffusion Models to World Models</li>
<li>The Confusing Instance Principle for Online Linear Quadratic Control</li>
<li>Long-Horizon Planning with Predictable Skills</li>
<li>Optimal discounting for offline input-driven MDP</li>
<li>DeepCubeAF: A Foundation Model for Generalizable Pathfinding Heuristics</li>
<li>A Timer-Enforced Hybrid Supervisor for Robust, Chatter-Free Policy Switching</li>
<li>Focused Skill Discovery: Using Per-Factor Empowerment to Control State Variables</li>
<li>Representation Learning and Skill Discovery with Empowerment</li>
<li>Compositional Instruction Following with Language Models and Reinforcement Learning</li>
<li>Composition and Zero-Shot Transfer with Lattice Structures in Reinforcement Learning</li>
<li>Double Horizon Model-Based Policy Optimization</li>
</ul></div>
<div class='bg-white p-4 rounded shadow'>
<h3 class='font-semibold text-lg mb-1'>Track 4: Evaluation, Benchmarks</h3>
<ul class='list-disc list-inside text-sm text-gray-700'>
<li>Which Experiences Are Influential for RL Agents? Efficiently Estimating The Influence of Experiences</li>
<li>Offline vs. Online Learning in Model-based RL: Lessons for Data Collection Strategies</li>
<li>Multi-Task Reinforcement Learning Enables Parameter Scaling</li>
<li>Benchmarking Massively Parallelized Multi-Task Reinforcement Learning for Robotics Tasks</li>
<li>PufferLib 2.0: Reinforcement Learning at 1M steps/s</li>
<li>Uncovering RL Integration in SSL Loss: Objective-Specific Implications for Data-Efficient RL</li>
<li>Benchmarking Partial Observability in Reinforcement Learning with a Suite of Memory-Improvable Domains</li>
<li>How Should We Meta-Learn Reinforcement Learning Algorithms?</li>
<li>AdaStop: adaptive statistical testing for sound comparisons of Deep RL agents</li>
<li>Mental Modelling of Reinforcement Learning Agents by Language Models</li>
</ul></div>
</div>
<h2 class='text-2xl font-bold text-gray-800'>Aug 7</h2>
<div class='grid grid-cols-1 md:grid-cols-2 lg:grid-cols-3 gap-4'>
<div class='bg-white p-4 rounded shadow'>
<h3 class='font-semibold text-lg mb-1'>Track 1: Deep RL</h3>
<ul class='list-disc list-inside text-sm text-gray-700'>
<li>Understanding the Effectiveness of Learning Behavioral Metrics in Deep Reinforcement Learning</li>
<li>Impoola: The Power of Average Pooling for Image-based Deep Reinforcement Learning</li>
<li>Eau De $Q$-Network: Adaptive Distillation of Neural Networks in Deep Reinforcement Learning</li>
<li>Disentangling Recognition and Decision Regrets in Image-Based Reinforcement Learning</li>
<li>Make the Pertinent Salient: Task-Relevant Reconstruction for Visual Control with Distractions</li>
<li>Pretraining Decision Transformers with Reward Prediction for In-Context Multi-task Structured Bandit Learning</li>
<li>Sampling from Energy-based Policies using Diffusion</li>
<li>Optimistic critics can empower small actors</li>
<li>Scalable Real-Time Recurrent Learning Using Columnar-Constructive Networks</li>
<li>AGaLiTe: Approximate Gated Linear Transformers for Online Reinforcement Learning</li>
</ul></div>
<div class='bg-white p-4 rounded shadow'>
<h3 class='font-semibold text-lg mb-1'>Track 2: Social and economic aspects, Neuroscience and cognitive science</h3>
<ul class='list-disc list-inside text-sm text-gray-700'>
<li>Pareto Optimal Learning from Preferences with Hidden Context</li>
<li>When and Why Hyperbolic Discounting Matters for Reinforcement Learning Interventions</li>
<li>Reinforcement Learning from Human Feedback with High-Confidence Safety Guarantees</li>
<li>Towards Large Language Models that Benefit for All: Benchmarking Group Fairness in Reward Models</li>
<li>Reinforcement Learning for Human-AI Collaboration via Probabilistic Intent Inference</li>
<li>High-Confidence Policy Improvement from Human Feedback</li>
<li>MixUCB: Enhancing Safe Exploration in Contextual Bandits with Human Oversight</li>
<li>Building Sequential Resource Allocation Mechanisms without Payments</li>
<li>From Explainability to Interpretability: Interpretable Reinforcement Learning Via Model Explanations</li>
<li>Learning Fair Pareto-Optimal Policies in Multi-Objective Reinforcement Learning</li>
<li>AI in a vat: Fundamental limits of efficient world modelling for safe agent sandboxing</li>
</ul></div>
<div class='bg-white p-4 rounded shadow'>
<h3 class='font-semibold text-lg mb-1'>Track 3: Exploration</h3>
<ul class='list-disc list-inside text-sm text-gray-700'>
<li>Uncertainty Prioritized Experience Replay</li>
<li>Pure Exploration for Constrained Best Mixed Arm Identification with a Fixed Budget</li>
<li>Learning to Explore in Diverse Reward Settings via Temporal-Difference-Error Maximization</li>
<li>Syllabus: Portable Curricula for Reinforcement Learning Agents</li>
<li>Exploration-Free Reinforcement Learning with Linear Function Approximation</li>
<li>Value Bonuses using Ensemble Errors for Exploration in Reinforcement Learning</li>
<li>Intrinsically Motivated Discovery of Temporally Abstract Graph-based Models of the World</li>
<li>An Optimisation Framework for Unsupervised Environment Design</li>
<li>Epistemically-guided forward-backward exploration</li>
<li>RLeXplore: Accelerating Research in Intrinsically-Motivated Reinforcement Learning</li>
</ul></div>
<div class='bg-white p-4 rounded shadow'>
<h3 class='font-semibold text-lg mb-1'>Track 4: Theoretical RL, Bandit algorithms</h3>
<ul class='list-disc list-inside text-sm text-gray-700'>
<li>A Finite-Time Analysis of Distributed Q-Learning</li>
<li>Finite-Time Analysis of Minimax Q-Learning</li>
<li>Improved Regret Bound for Safe Reinforcement Learning via Tighter Cost Pessimism and Reward Optimism</li>
<li>Non-Stationary Latent Auto-Regressive Bandits</li>
<li>A Finite-Sample Analysis of an Actor-Critic Algorithm for Mean-Variance Optimization in a Discounted MDP</li>
<li>Leveraging priors on distribution functions for multi-arm bandits</li>
<li>Multi-task Representation Learning for Fixed Budget Pure-Exploration in Linear and Bilinear Bandits</li>
<li>On Slowly-varying Non-stationary Bandits</li>
<li>Empirical Bound Information-Directed Sampling</li>
<li>Thompson Sampling for Constrained Bandits</li>
<li>Achieving Limited Adaptivity for Multinomial Logistic Bandits</li>
</ul></div>
</div>
<h2 class='text-2xl font-bold text-gray-800'>Aug 8</h2>
<div class='grid grid-cols-1 md:grid-cols-2 lg:grid-cols-3 gap-4'>
<div class='bg-white p-4 rounded shadow'>
<h3 class='font-semibold text-lg mb-1'>Track 1: RL algorithms, Deep RL</h3>
<ul class='list-disc list-inside text-sm text-gray-700'>
<li>Bayesian Meta-Reinforcement Learning with Laplace Variational Recurrent Networks</li>
<li>Cascade - A sequential ensemble method for continuous control tasks</li>
<li>HANQ: Hypergradients, Asymmetry, and Normalization for Fast and Stable Deep $Q$-Learning</li>
<li>Rectifying Regression in Reinforcement Learning</li>
<li>Efficient Morphology-Aware Policy Transfer to New Embodiments</li>
<li>Finer Behavioral Foundation Models via Auto-Regressive Features and Advantage Weighting</li>
<li>Concept-Based Off-Policy Evaluation</li>
<li>Multiple-Frequencies Population-Based Training</li>
<li>AVG-DICE: Stationary Distribution Correction by Regression</li>
<li>Deep Reinforcement Learning with Gradient Eligibility Traces</li>
<li>Iterated Q-Network: Beyond One-Step Bellman Updates in Deep Reinforcement Learning</li>
</ul></div>
<div class='bg-white p-4 rounded shadow'>
<h3 class='font-semibold text-lg mb-1'>Track 2: Applied RL</h3>
<ul class='list-disc list-inside text-sm text-gray-700'>
<li>Action Mapping for Reinforcement Learning in Continuous Environments with Constraints</li>
<li>Chargax: A JAX Accelerated EV Charging Simulator</li>
<li>WOFOSTGym: A Crop Simulator for Learning Annual and Perennial Crop Management Strategies</li>
<li>Drive Fast, Learn Faster: On-Board RL for High Performance Autonomous Racing</li>
<li>Quantitative Resilience Modeling for Autonomous Cyber Defense</li>
<li>Multi-Agent Reinforcement Learning for Inverse Design in Photonic Integrated Circuits</li>
<li>Gaussian Process Q-Learning for Finite-Horizon Markov Decision Process</li>
<li>Hybrid Classical/RL Local Planner for Ground Robot Navigation</li>
<li>V-Max: Making RL Practical for Autonomous Driving</li>
<li>Shaping Laser Pulses with Reinforcement Learning</li>
<li>Learning Sub-Second Routing Optimization in Computer Networks requires Packet-Level Dynamics</li>
</ul></div>
<div class='bg-white p-4 rounded shadow'>
<h3 class='font-semibold text-lg mb-1'>Track 3: Multi-agent RL</h3>
<ul class='list-disc list-inside text-sm text-gray-700'>
<li>Reinforcement Learning for Finite Space Mean-Field Type Game</li>
<li>Collaboration Promotes Group Resilience in Multi-Agent RL</li>
<li>Foundation Model Self-Play: Open-Ended Strategy Innovation via Foundation Models</li>
<li>Hierarchical Multi-agent Reinforcement Learning for Cyber Network Defense</li>
<li>Efficient Information Sharing for Training Decentralized Multi-Agent World Models</li>
<li>Adaptive Reward Sharing to Enhance Learning in the Context of Multiagent Teams</li>
<li>Seldonian Reinforcement Learning for Ad Hoc Teamwork</li>
<li>Joint-Local Grounded Action Transformation for Sim-to-Real Transfer in Multi-Agent Traffic Control</li>
<li>TransAM: Transformer-Based Agent Modeling for Multi-Agent Systems via Local Trajectory Encoding</li>
<li>PEnGUiN: Partially Equivariant Graph NeUral Networks for Sample Efficient MARL</li>
<li>Human-Level Competitive Pokémon via Scalable Offline Reinforcement Learning with Transformers</li>
</ul></div>
<div class='bg-white p-4 rounded shadow'>
<h3 class='font-semibold text-lg mb-1'>Track 4: Foundations</h3>
<ul class='list-disc list-inside text-sm text-gray-700'>
<li>Effect of a slowdown correlated to the current state of the environment on an asynchronous learning architecture</li>
<li>Average-Reward Soft Actor-Critic</li>
<li>Your Learned Constraint is Secretly a Backward Reachable Tube</li>
<li>Recursive Reward Aggregation</li>
<li>On the Effect of Regularization in Policy Mirror Descent</li>
<li>Investigating the Utility of Mirror Descent in Off-policy Actor-Critic</li>
<li>Rethinking the Foundations for Continual Reinforcement Learning</li>
<li>An Analysis of Action-Value Temporal-Difference Methods That Learn State Values</li>
<li>Reinforcement Learning with Adaptive Temporal Discounting</li>
<li>Learning in complex action spaces without policy gradients</li>
</ul></div>
</div>
</body></html>