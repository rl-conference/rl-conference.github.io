<!DOCTYPE html>
<html lang="en">

<head>
  <!-- Google Tag Manager -->
<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
  new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
  j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
  'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
  })(window,document,'script','dataLayer','GTM-WJ8BP2WZ');</script>
  <!-- End Google Tag Manager -->
  <meta charset="UTF-8" />
  <title>RLC 2025 - Outstanding Paper Awards</title>
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Rubik:ital,wght@0,300..900;1,300..900&display=swap"
    rel="stylesheet">
  <link
    href="https://fonts.googleapis.com/css2?family=Bai+Jamjuree:wght@200..700&family=Rubik:ital,wght@0,300..900;1,300..900&display=swap"
    rel="stylesheet">
  <link href="build.css" rel="stylesheet">
  <link rel="icon" type="image/png" href="/favicon-48x48.png" sizes="48x48" />
  <link rel="icon" type="image/svg+xml" href="/favicon.svg" />
  <link rel="shortcut icon" href="/favicon.ico" />
  <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png" />
  <meta name="apple-mobile-web-app-title" content="RLC" />
  <link rel="manifest" href="/site.webmanifest" />
</head>

<body class="bg-rlgrey font-rubik text-rldarkblue-900 p-6">
  <!-- Google Tag Manager (noscript) -->
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-WJ8BP2WZ"
  height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
  <!-- End Google Tag Manager (noscript) -->

  <!-- Include Menu -->
  <script src="jquery.js"></script>
  <script src="data.js"></script>

  <div class="container mx-auto">
    <div class="flex flex-col min-h-screen justify-between">
      <div>
        <div class="m-2 grid grid-cols-3 items-center rounded-md mt-4 p-2 border-0 border-rldarkblue-900">
          <div class="p-2 w-full rounded-md">
            <div class="hidden lg:block max-w-60">
              <a href="index.html"><img alt="Company logo" src="data/logos/rlc-logo.svg" /></a>
            </div>
            <div class="block pt-1 lg:hidden max-w-60">
              <a href="index.html"><img alt="Company logo" src="data/logos/rlc-logo.svg" /></a>
            </div>
          </div>
          <div></div>
          <div id="largeMenu" class="p-2 m-1 w-full hidden lg:block rounded-md"></div>
          <div id="collapsedMenu" class="p-2 m-1 w-full block lg:hidden rounded-md">
            <div class="relative flex flex-row-reverse">
              <svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5"
                stroke="currentColor"
                class="size-10 p-1 font-rubik text-xl m-1 text-rldarkblue-900 hover:text-rldarkblue-500 hover:cursor-pointer"
                onclick="showMenu()">
                <path stroke-linecap="round" stroke-linejoin="round"
                  d="M3.75 5.25h16.5m-16.5 4.5h16.5m-16.5 4.5h16.5m-16.5 4.5h16.5" />
              </svg>
              <div id="collapsedMenuItems"
                class="absolute top-10 z-10 hidden w-40 shadow-md bg-rlgrey/100 backdrop-blur-md rounded-md p-4 m-4 grid grid-cols-1 border border-black">
              </div>
            </div>
          </div>
        </div>

        <!-- PAGE TITLE -->
        <h1 class="md:text-4xl text-2xl font-bai text-center text-rldarkblue-900 mb-10 mt-10">Announcing the RLC 2025 Outstanding Paper Awards</h1>

        <!-- MAIN CONTENT -->
        <div class="max-w-4xl mx-auto p-6">
          <div class="bg-white rounded-lg  p-8 mb-8">
            <p class="text-lg text-gray-700 mb-6">
              We are honoured to announce the winners of the Outstanding Paper Awards at the Second Reinforcement Learning Conference. Like last year, papers are awarded based on specific aspects of their contribution. We describe the process we used to select the awarded papers below.
            </p>
            <p class="text-lg text-gray-700 mb-8">
                This year's awards consist of eight papers. The awarded papers are listed below in alphabetical order by award name. Congratulations to all the authors!

            </p>
          </div>

          <!-- AWARDED PAPERS -->
          <div class="space-y-8">
            <!-- Applications of Reinforcement Learning -->
            <div class="bg-rldarkblue-50/50 rounded-lg p-6 border-blue">
              <h3 class="text-xl font-semibold text-rldarkblue-900 mb-4 text-center">Applications of Reinforcement Learning</h3>
              <div class="bg-white rounded-lg p-4 mb-4">
                <h4 class="font-semibold text-lg mb-2">
                  <a href="https://openreview.net/forum?id=x00VCsuHAb" class="text-blue hover:text-rldarkblue-500 underline" target="_blank">
                    WOFOSTGym: A Crop Simulator for Learning Annual and Perennial Crop Management Strategies
                  </a>
                </h4>
                <p class="text-gray-600 mb-2">
                  <strong>Authors:</strong> William Solow, Sandhya Saisubramanian, Alan Fern
                </p>
                <p class="text-gray-700 italic">
                  This is the first paper to introduce a high-fidelity crop simulation environment that uniquely supports both annual and perennial crops in multi-farm settings. This work addresses a critical gap in RL applications for agriculture, enabling the development of advanced agromanagement strategies under realistic constraints. Its innovative modifications to the WOFOST model and use of Bayesian Optimization significantly advance the practical utility and scientific rigor of RL in this vital domain.
                </p>
              </div>
            </div>

            <!-- Emerging Topics in Reinforcement Learning -->
            <div class="bg-rldarkblue-50/50 rounded-lg p-6 border-blue">
              <h3 class="text-xl font-semibold text-rldarkblue-900 mb-4 text-center">Emerging Topics in Reinforcement Learning</h3>
              <div class="bg-white rounded-lg p-4 mb-4">
                <h4 class="font-semibold text-lg mb-2">
                  <a href="https://openreview.net/forum?id=XZBYLXNGjT" class="text-blue hover:text-rldarkblue-500 underline" target="_blank">
                    Towards Improving Reward Design in RL: A Reward Alignment Metric for RL Practitioners
                  </a>
                </h4>
                <p class="text-gray-600 mb-2">
                  <strong>Authors:</strong> Calarina Muslimani, Kerrick Johnstonbaugh, Suyog Chandramouli, Serena Booth, W. Bradley Knox, Matthew E. Taylor
                </p>
                <p class="text-gray-700 italic">
                  This paper makes an exceptional contribution to the emerging field of reinforcement learning from human feedback by introducing the Trajectory Alignment Coefficient, a new metric for evaluating how well a reward function aligns with human preferences. This work stands out for its practical impact, demonstrating that the metric reduces cognitive workload and increases the success rate of selecting performant reward functions for RL practitioners. By addressing the critical challenge of reward design in real-world applications, this paper offers a pioneering tool that is poised to significantly influence future research in the space of human-AI interaction.
                </p>
              </div>
            </div>

            <!-- Empirical Reinforcement Learning Research -->
            <div class="bg-rldarkblue-50/50 rounded-lg p-6 border-blue">
              <h3 class="text-xl font-semibold text-rldarkblue-900 mb-4 text-center">Empirical Reinforcement Learning Research</h3>
              <div class="bg-white rounded-lg p-4 mb-4">
                <h4 class="font-semibold text-lg mb-2">
                  <a href="https://openreview.net/forum?id=H3jcTxcvvJ" class="text-blue hover:text-rldarkblue-500 underline" target="_blank">
                    Mitigating Suboptimality of Deterministic Policy Gradients in Complex Q-functions
                  </a>
                </h4>
                <p class="text-gray-600 mb-2">
                  <strong>Authors:</strong> Ayush Jain, Norio Kosaka, Xinhu Li, Kyung-Min Kim, Erdem Biyik, Joseph J Lim
                </p>
                <p class="text-gray-700 italic">
                  This paper addresses the fundamental challenge of local optima in complex Q-functions, a key problem for off-policy actor-critic methods in real-world applications. It proposes Successive Actors for Value Optimization (SAVO), an architecture that uses multiple actors and progressively simplified Q-landscapes to escape suboptimal policies. The work is a model of empirical rigor, demonstrating SAVO's superior performance across a diverse suite of challenging tasks, including dexterous manipulation and large-scale recommender systems.
                </p>
              </div>
            </div>

            <!-- Resourcefulness in Reinforcement Learning -->
            <div class="bg-rldarkblue-50/50 rounded-lg p-6 border-blue">
              <h3 class="text-xl font-semibold text-rldarkblue-900 mb-4 text-center">Resourcefulness in Reinforcement Learning</h3>
              <div class="bg-white rounded-lg p-4 mb-4">
                <h4 class="font-semibold text-lg mb-2">
                  <a href="https://openreview.net/forum?id=qRyteMTgn0" class="text-blue hover:text-rldarkblue-500 underline" target="_blank">
                    PufferLib 2.0: Reinforcement Learning at 1M steps/s
                  </a>
                </h4>
                <p class="text-gray-600 mb-2">
                  <strong>Authors:</strong> Joseph Suarez
                </p>
                <p class="text-gray-700 italic">
                  This paper introduces PufferLib 2.0, a resourceful toolkit that tackles the high computational cost of modern reinforcement learning research. By offering a suite of C-based environments and fast vectorization methods, PufferLib enables high-speed simulation, making large-scale experimentation accessible on a single desktop. This project's focus on a smooth and efficient development experience for complex environments marks a significant contribution to fostering frugal yet rigorous empirical research.
                </p>
              </div>
            </div>

            <!-- Scientific Understanding in Reinforcement Learning -->
            <div class="bg-rldarkblue-50/50 rounded-lg p-6 border-blue">
              <h3 class="text-xl font-semibold text-rldarkblue-900 mb-4 text-center">Scientific Understanding in Reinforcement Learning</h3>
              <div class="bg-white rounded-lg p-4 mb-4">
                <h4 class="font-semibold text-lg mb-2">
                  <a href="https://openreview.net/forum?id=eBWwBIFV7T" class="text-blue hover:text-rldarkblue-500 underline" target="_blank">
                    Multi-Task Reinforcement Learning Enables Parameter Scaling
                  </a>
                </h4>
                <p class="text-gray-600 mb-2">
                  <strong>Authors:</strong> Reginald McLean, Evangelos Chatzaroulas, J K Terry, Isaac Woungang, Nariman Farsad, Pablo Samuel Castro
                </p>
                <p class="text-gray-700 italic">
                  This paper significantly advances the scientific understanding of multi-task reinforcement learning by demonstrating that performance gains often attributed to complex architectures are primarily a result of parameter scaling. It reveals that naïvely scaling up a simple feed-forward architecture can outperform more sophisticated designs. Furthermore, the work uncovers a novel relationship where increasing task diversity can mitigate plasticity loss in larger models, providing a clear, evidence-based understanding of the drivers of performance in multi-task RL.
                </p>
              </div>
              <div class="bg-white rounded-lg p-4 mb-4">
                <h4 class="font-semibold text-lg mb-2">
                  <a href="https://openreview.net/forum?id=jKzQ6af2DU" class="text-blue hover:text-rldarkblue-500 underline" target="_blank">
                    How Should We Meta-Learn Reinforcement Learning Algorithms?
                  </a>
                </h4>
                <p class="text-gray-600 mb-2">
                  <strong>Authors:</strong> Alexander David Goldie, Zilin Wang, Jakob Nicolaus Foerster, Shimon Whiteson
                </p>
                <p class="text-gray-700 italic">
                  This paper provides a crucial empirical study that significantly advances the scientific understanding of how to meta-learn reinforcement learning (RL) algorithms. It directly compares several meta-learning approaches—including black-box learning, distillation, and LLM proposals—and offers a clear analysis of their trade-offs in terms of performance, sample cost, and interpretability. The findings provide actionable recommendations to help researchers design more efficient and effective approaches for meta meta-learning RL algorithms.
                </p>
              </div>
            </div>

            <!-- Theory of Reinforcement Learning -->
            <div class="bg-rldarkblue-50/50 rounded-lg p-6 border-blue">
              <h3 class="text-xl font-semibold text-rldarkblue-900 mb-4 text-center">Theory of Reinforcement Learning</h3>
              <div class="bg-white rounded-lg p-4 mb-4">
                <h4 class="font-semibold text-lg mb-2">
                  <a href="https://openreview.net/forum?id=LZAafvwVMa" class="text-blue hover:text-rldarkblue-500 underline" target="_blank">
                    Deep Reinforcement Learning with Gradient Eligibility Traces
                  </a>
                </h4>
                <p class="text-gray-600 mb-2">
                  <strong>Authors:</strong> Esraa Elelimy, Brett Daley, Andrew Patterson, Marlos C. Machado, Adam White, Martha White
                </p>
                <p class="text-gray-700 italic">
                  This paper presents a foundational theoretical contribution to deep reinforcement learning by extending the Generalized Projected Bellman Error (GPBE) to a multi-step objective, GPBE(λ), using λ-returns. It derives and evaluates three novel Gradient TD algorithms for this new objective, providing both forward-view and backward-view formulations compatible with modern deep RL practices. This work offers a principled approach to stable off-policy learning, tackling the divergence issues of semi-gradient methods and demonstrating superior performance in practice.
                </p>
              </div>
            </div>

            <!-- Tooling, Environments, and Evaluation for Reinforcement Learning -->
            <div class="bg-rldarkblue-50/50 rounded-lg p-6 border-blue">
              <h3 class="text-xl font-semibold text-rldarkblue-900   mb-4 text-center">Tooling, Environments, and Evaluation for Reinforcement Learning</h3>
              <div class="bg-white rounded-lg p-4 mb-4">
                <h4 class="font-semibold text-lg mb-2">
                  <a href="https://openreview.net/forum?id=0LFJnnMKeT" class="text-blue hover:text-rldarkblue-500 underline" target="_blank">
                    Syllabus: Portable Curricula for Reinforcement Learning Agents
                  </a>
                </h4>
                <p class="text-gray-600 mb-2">
                  <strong>Authors:</strong> Ryan Sullivan, Ryan Pégoud, Ameen Ur Rehman, Xinchen Yang, Junyun Huang, Aayush Verma, Nistha Mitra, John P Dickerson
                </p>
                <p class="text-gray-700 italic">
                  This paper introduces Syllabus, a groundbreaking library that provides portable curriculum learning algorithms and infrastructure, addressing a critical gap in standard RL tooling. By defining a universal API, Syllabus enables researchers to easily integrate advanced curriculum learning methods into nearly any RL library. This work significantly lowers the barrier to entry for a crucial component of modern RL, encouraging research on more complex, challenging environments like NetHack and Neural MMO.
                </p>
              </div>
            </div>
          </div>

          <!-- AWARD CATEGORIES SECTION -->
          <div class="bg-white rounded-lg  p-8 mt-12">
            <h2 class="text-2xl font-semibold text-blue mb-6">Award Categories</h2>
            <p class="text-lg text-gray-700 mb-6">
              Following the first edition of RLC, the "Outstanding Paper Award" at RLC 2025 is different from what is traditionally done in machine learning conferences. We do not award papers for being the overall "best" papers in a conference, instead we award papers for making significant contributions to specific aspects of research. We believe such an approach will be more <em>inclusive</em>, it will celebrate the <em>diverse</em> types of scientific contribution one can make in the field, and it will give a more <em>equal</em> opportunity for different types of papers to be awarded. The idea is to award papers for excelling in what they propose to do.
            </p>
            <p class="text-lg text-gray-700 mb-6">
              This year, we considered awards for nine categories. Relative to last year, we introduced two new categories, one for RL contributions to natural sciences and one for contributions to emerging topics in RL.
            </p>
            <p class="text-lg text-gray-700 mb-6">
              We have also changed the name of one of last year's award categories from Outstanding Paper Award on Support Tools for RL Research to <strong>Outstanding Paper Award on Tooling, Environments, and Evaluation for Reinforcement Learning</strong> to make it more descriptive.
            </p>
            <p class="text-lg text-gray-700 mb-6">
              Finally, we refined some of the descriptions of last year's categories to make them more comprehensive and better reflect the breadth of contributions we aim to consider for that category. A more detailed description of each category is available at the end of this document. Importantly, no award is more prestigious than the other.
            </p>
            <p class="text-lg text-gray-700 mb-4">Full list of outstanding awards we considered for RLC 2025 (in alphabetical order):</p>
            <ul class="list-disc list-inside space-y-2 text-lg text-gray-700 ml-4">
              <li>Outstanding Paper Award on Applications of Reinforcement Learning</li>
              <li>Outstanding Paper Award on Emerging Topics in Reinforcement Learning</li>
              <li>Outstanding Paper Award on Empirical Reinforcement Learning Research</li>
              <li>Outstanding Paper Award on Pioneering Vision in Reinforcement Learning</li>
              <li>Outstanding Paper Award on Reinforcement Learning Contributions to Natural Sciences</li>
              <li>Outstanding Paper Award on Resourcefulness in Reinforcement Learning</li>
              <li>Outstanding Paper Award on Scientific Understanding in Reinforcement Learning</li>
              <li>Outstanding Paper Award on the Theory of Reinforcement Learning</li>
              <li>Outstanding Paper Award on Tooling, Environments, and Evaluation for Reinforcement Learning</li>
            </ul>
          </div>

          <!-- SELECTION PROCESS SECTION -->
          <div class="bg-white rounded-lg  p-8 mt-8">
            <h2 class="text-2xl font-semibold text-blue mb-6">Selection Process</h2>
            <p class="text-lg text-gray-700 mb-6">
              For RLC 2025, we've updated our award process while keeping the core philosophy from last year. We believe in recognizing papers for their <strong>specific strengths</strong>, rather than for a single, overall score based on perceived novelty or impact. This means a paper can still win an award even if it has minor flaws in areas outside of its award-winning contribution. Our goal is to highlight papers that excel in one specific aspect, doing that one thing exceptionally well.
            </p>
            <p class="text-lg text-gray-700 mb-6">Based on feedback and new insights, we've made a few changes this year:</p>
            <ul class="list-disc list-inside space-y-2 text-lg text-gray-700 ml-4 mb-6">
              <li><strong>Author Eligibility:</strong> We've removed the rule that excluded papers with conference organizers as co-authors (with the exception of the awards co-chairs). We made this change to ensure that student first authors aren't unfairly penalized and to avoid negatively impacting researchers from underrepresented communities.</li>
              <li><strong>Reviewer Input:</strong> Instead of collecting nominations from reviewers, we placed more weight on the review scores to ensure award-winning papers had strong support from the reviewers.</li>
            </ul>
            <p class="text-lg text-gray-700 mb-6">To select the outstanding paper awards, we used the following selection criteria:</p>
            <ul class="list-disc list-inside space-y-2 text-lg text-gray-700 ml-4 mb-6">
              <li><strong>Methodological Rigor:</strong> The work must demonstrate outstanding rigor appropriate for its claims, such as error-free proofs for theory or robust, reproducible, and well-controlled experiments for empirical work.</li>
              <li><strong>Core RL Contribution:</strong> The paper must fundamentally advance Reinforcement Learning theory, algorithms, core concepts, or applications, rather than simply being inspired by RL concepts but using different techniques.</li>
              <li><strong>Clarity and Quality:</strong> The paper must be clear, well-written, and effectively communicate its claims, as well as the process and evidence in support of these claims.</li>
            </ul>
            <p class="text-lg text-gray-700 mb-6">Our selection process was a two-stage, independent effort between the two of us.</p>
            <ul class="list-disc list-inside space-y-2 text-lg text-gray-700 ml-4 mb-6">
              <li><strong>Stage 1:</strong> We independently reviewed the abstracts and meta-reviews of all 115 accepted papers. We then met to narrow down the pool to about 30 papers for a closer look.</li>
              <li><strong>Stage 2:</strong> We read the reviews for each of the 30 papers and looked at key aspects of the papers themselves. This led to a shortlist of 0-3 papers per category. We then rated each of these papers independently on a scale of 1 to 5. Although our individual ratings varied slightly, we were in complete agreement on the final award recipients.</li>
            </ul>
            <p class="text-lg text-gray-700 mb-6">
              This approach enabled us to recognize a more diverse range of papers, celebrating contributions that might have been overlooked by traditional review processes. We awarded papers with and without theoretical results, with simple and complex ideas, and with both small and large-scale experiments. This diversity is a major strength for our community.
            </p>
            <p class="text-lg text-gray-700 mb-6">
              This year, we awarded <strong>two papers for Scientific Understanding in RL</strong> but <strong>no awards for Pioneering Vision in RL or RL Contributions to Natural Sciences</strong>. For future RLC conferences, we'd love to see more submissions in these areas.
            </p>
            <p class="text-lg text-gray-700 font-semibold">Congratulations to all the authors!</p>
          </div>

          <!-- DESCRIPTION OF AWARD CATEGORIES SECTION -->
          <div class="bg-white rounded-lg  p-8 mt-8">
            <h2 class="text-2xl font-semibold text-rldarkblue-900 text-center mb-6">Description of Award Categories</h2>
            
            <div class="space-y-6">
              <div>
                <h3 class="text-xl font-semibold text-rldarkblue-900 mb-3 m-1 p-1">Outstanding Paper Award on Applications of Reinforcement Learning</h3>
                <p class="text-gray-700 p-1 m-1">
                  This award aims to acknowledge papers that demonstrate substantial progress on the application of reinforcement learning to complex, real-world problems. This award seeks to highlight groundbreaking work formulating real-world problems using the reinforcement learning framework, introducing a new application domain or challenge to reinforcement learning, or developing reinforcement learning methods that make significant progress on practical scenarios. The papers should display a notable level of practical utility and uphold a high standard of scientific rigor.
                </p>
              </div>

              <div>
                <h3 class="text-xl font-semibold text-rldarkblue-900 mb-3 m-1 p-1">Outstanding Paper Award on Emerging Topics in Reinforcement Learning</h3>
                <p class="text-gray-700 p-1 m-1">
                  This award acknowledges papers that make exceptional contributions to emerging topics in the field of reinforcement learning. This category seeks to recognize groundbreaking work on novel and forward-thinking ideas connecting RL with broader trends in machine learning that are poised to become central to future research and applications. Examples of such topics include, but are not limited to, foundation models, world models, and reinforcement learning from human feedback. The papers should demonstrate innovative approaches and the potential to significantly influence the direction of reinforcement learning research by opening up new avenues of exploration.
                </p>
              </div>

              <div>
                <h3 class="text-xl font-semibold text-rldarkblue-900 mb-3 m-1 p-1">Outstanding Paper Award on Empirical Reinforcement Learning Research</h3>
                <p class="text-gray-700 p-1 m-1">
                  This award recognizes papers that make significant contributions to the empirical aspects of reinforcement learning research. Examples include addressing fundamental practical challenges in reinforcement learning, introducing new empirical practices, methodologies, benchmarks, evaluation metrics, and visualization techniques, and providing tools and frameworks that will further enable empirical research. These papers should show a high standard of practical relevance and experimental rigor.
                </p>
              </div>

              <div>
                <h3 class="text-xl font-semibold text-rldarkblue-900 mb-3 m-1 p-1">Outstanding Paper Award on Pioneering Vision in Reinforcement Learning</h3>
                <p class="text-gray-700 p-1 m-1">
                  This award highlights papers that stand out with their forward-thinking vision and blue sky ideas in the field of reinforcement learning. The papers awarded in this category will present groundbreaking, visionary ideas, theories, or techniques in reinforcement learning, potentially reshaping current perspectives or opening new avenues for research and applications. The papers must demonstrate originality, creativity, and the potential to inspire transformative advancements in reinforcement learning.
                </p>
              </div>

              <div>
                <h3 class="text-xl font-semibold text-rldarkblue-900 mb-3 m-1 p-1">Outstanding Paper Award on Resourcefulness in Reinforcement Learning</h3>
                <p class="text-gray-700 p-1 m-1">
                  This award honors papers that demonstrate resourcefulness in empirical research. These are papers that overcome the high computational cost of empirical research in reinforcement learning in ingenious ways, promoting more frugal empirical research. Examples include showcasing original, cost-effective methodologies, and resource-efficient experimental designs. The papers should embody high standards of creativity and practicality without sacrificing experimental rigor.
                </p>
              </div>

              <div>
                <h3 class="text-xl font-semibold text-rldarkblue-900 mb-3 m-1 p-1">Outstanding Paper Award on Reinforcement Learning Contributions to Natural Sciences</h3>
                <p class="text-gray-700 p-1 m-1">
                    This award recognizes papers that make exceptional contributions to the advancement of natural sciences through reinforcement learning. Awarded papers will demonstrate how reinforcement learning methods have been effectively applied to generate new insights, drive discovery, or model complex phenomena in fields such as neuroscience, cognitive science, psychology, biology, animal learning, or related disciplines. These papers should exemplify scientific rigor, cross-disciplinary innovation, and a clear impact on our understanding of the natural world. They are expected to bridge reinforcement learning and natural sciences in a way that fosters new scientific methodologies or enhances existing ones.
                </p>
              </div>

              <div>
                <h3 class="text-xl font-semibold text-rldarkblue-900 mb-3 m-1 p-1">Outstanding Paper Award on Scientific Understanding in Reinforcement Learning</h3>
                <p class="text-gray-700 p-1 m-1">
                    This award celebrates papers that significantly advance scientific understanding in the domain of reinforcement learning. It encourages the development of well-founded, clear understanding of the behavior of existing algorithms or the nuances of different problem formulations or different environments. Awarded papers will fill gaps in our understanding of the field; they will bring clarity to unexplored aspects of existing algorithms, they will provide evidence to dispute common assumptions, or they will better justify common practices in the field. They should also demonstrate excellence in scientific rigor and clarity of exposition, with very well-defined claims.

                </p>
              </div>

              <div>
                <h3 class="text-xl font-semibold text-rldarkblue-900 mb-3 m-1 p-1">Outstanding Paper Award on the Theory of Reinforcement Learning</h3>
                <p class="text-gray-700 p-1 m-1">
                    This award acknowledges papers that provide exceptional theoretical contributions to the field of reinforcement learning. Examples include theoretical unifications, new theoretical frameworks or formalisms, mathematical models, results, and theoretical insights into existing RL practices. The papers must exhibit a high level of technical proficiency and innovation.

                </p>
              </div>

              <div>
                <h3 class="text-xl font-semibold text-rldarkblue-900 mb-3 m-1 p-1">Outstanding Paper Award on Tooling, Environments, and Evaluation for Reinforcement Learning</h3>
                <p class="text-gray-700 p-1 m-1">
                    Outstanding Paper Award on Tooling, Environments, and Evaluation for Reinforcement Learning Research recognizes papers that make significant contributions to support tools for reinforcement learning research. Examples include introducing new environments, datasets, benchmarks, evaluation metrics, visualization techniques, or frameworks, libraries, and tools that will further enable empirical research in reinforcement learning. These papers should show a high standard of practical relevance, accessibility, ease of use and reproducibility.
                </p>
              </div>
            </div>
          </div>
        </div>
      </div>

      <!-- FOOTER -->
      <div class="grid grid-cols-2 items-center mt-12">
        <div id="footerText" class="p-2 m-1 w-full rounded-md text-rldarkblue-900 font-roboto text-xs sm:text-base">
        </div>
        <div class="p-2 m-1 w-full">
          <div class="flex flex-row-reverse p-2 ml-auto max-w-60">
            <div><img alt="Company logo" class="p-1 m-1 w-60" src="data/logos/rlc-logo.svg" /></div>
          </div>
        </div>
      </div>
    </div>
  </div>

</body>
<!-- Load menu functionality -->
<script src="menu.js"></script>

</html>